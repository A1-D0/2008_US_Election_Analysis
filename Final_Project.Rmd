---
output:
  pdf_document: default
  html_document: default
---
---
title: "Exploration of the Influential Factor of the Election Results in 2008"
author: "Daisy, Mike, and Osvaldo"
output: pdf_document
date: "2024-12-05"
---

```{r setup, echo=FALSE}
# consider removing code from knit file
knitr::opts_chunk$set(echo = FALSE)

```

```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
library(Stat2Data)
library(ggplot2)
library(dplyr)
library(car)
data("Election08")
```

# Abstract
This study examines state-level factors predicting whether Barack Obama or John McCain won the 2008 U.S. presidential election. By leveraging both simple and multiple linear regression models, we aim to determine the most effective linear model to adopt for our final logistic regression model. In our context, we implement logistic regression models to analyze and compare predictors, identifying the most significant contributors to election outcomes. As a result, we reveal which predictors are most influential for predicting who won the 2008 U.S. election.

# Introduction
The goal of our study is to use logistic regression to determine which state-level socioeconomic and political factors most effectively predict the 2008 U.S. election results based on the Election08 dataset. This analysis helps us understand the relationships among predictors such as income, education levels, and political leanings.

# Research Question
Among political leaning, education level, and income, what is the most influential factor in predicting the presidential election result in 2008?
Is the multiple regression model more effective in predicting the election result than the single regression models?

# Expected Findings
Note the target population.

# Methodology
The dataset of reference contains information from all 50 states and the District of Columbia for the 2008 U.S. presidential election. This analysis consists of 5 models, 4 of which investigate if **Income**, **HS**, **BA**, and **Dem.Rep** is associated with the odds that Obama (Democrat) wins state in 2008 (**ObamaWin** = 1), and 1 of which is an interaction model that investigates the joint effect of these four variables.


# Dataset Overview

## Description of the dataset
It is important to understand the factors that influenced voting behavior across different states in analyzing the 2008 U.S. presidential election results. We are interested in exploring how demographic variables and educational attainment correlate with the likelihood of Barack Obama's winning in various states.

## Definition of variables
The dataset includes 51 observations (50 states + DC) with variables related to income, education, and political alignment. The dependent variable is `ObamaWin`, a binary outcome (1 = Obama won the state, 0 = McCain won the state).
Independent variables are:  
1. **Income**: Per capita income in the state as of 2007 (in US dollars).  
2. **HS**: Percentage of adults with at least a high school education.  
3. **BA**: Percentage of adults with at least a college education.  
4. **Dem.Rep**: Difference in percent Democrat and percent Republican (according to 2008 Gallup survey).

```{r}
data(Election08)
str(Election08)
summary(Election08)

```


# Data Preparation
```{r}
# Data cleaning and summary
summary(Election08)

# Visualize relationships
# Boxplot: Income vs ObamaWin
ggplot(Election08, aes(x = Income, y = factor(ObamaWin), fill = factor(ObamaWin))) +
  geom_boxplot() +
  labs(title = "Income vs ObamaWin", x = "Income", y = "Obama Win (1=Yes, 0=No)") +
  scale_fill_manual(values = c("red", "blue")) +
  theme_minimal()

# Boxplot: HS vs ObamaWin
ggplot(Election08, aes(x = HS, y = factor(ObamaWin), fill = factor(ObamaWin))) +
  geom_boxplot() +
  labs(title = "High School Education (%) vs ObamaWin", x = "HS (%)", y = "Obama Win (1=Yes, 0=No)") +
  scale_fill_manual(values = c("red", "blue")) +
  theme_minimal()

# Boxplot: BA vs ObamaWin
ggplot(Election08, aes(x = BA, y = factor(ObamaWin), fill = factor(ObamaWin))) +
  geom_boxplot() +
  labs(title = "Bachelor's Degree (%) vs ObamaWin", x = "BA (%)", y = "Obama Win (1=Yes, 0=No)") +
  scale_fill_manual(values = c("red", "blue")) +
  theme_minimal()

# Boxplot: Dem.Rep vs ObamaWin
ggplot(Election08, aes(x = Dem.Rep, y = factor(ObamaWin), fill = factor(ObamaWin))) +
  geom_boxplot() +
  labs(title = "Democrat-Republican Difference vs ObamaWin", x = "Dem.Rep (%)", y = "Obama Win (1=Yes, 0=No)") +
  scale_fill_manual(values = c("red", "blue")) +
  theme_minimal()

```


# Statistical Analysis

## Check Conditions
The conditions for logistic regressions are linearity, randomness, and independence.
Linearity: this means that there should exist a linear relationship between log(odds) and the predictor.
The process of data collection is assumed to be random, and each observation is assumed to be independent.

```{r}

logistic_model <- glm(ObamaWin ~ Income + HS + BA + Dem.Rep, data = Election08, family = "binomial")

Election08$logit <- predict(logistic_model, type = "link")  # Logit values

ggplot(Election08, aes(x = Income, y = logit)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, color = "blue") +
  labs(title = "Linearity Check: Income vs Logit", x = "Income", y = "Logit")

ggplot(Election08, aes(x = HS, y = logit)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, color = "blue") +
  labs(title = "Linearity Check: HS vs Logit", x = "HS", y = "Logit")

ggplot(Election08, aes(x = BA, y = logit)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, color = "blue") +
  labs(title = "Linearity Check: BA vs Logit", x = "BA", y = "Logit")

ggplot(Election08, aes(x = Dem.Rep, y = logit)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, color = "blue") +
  labs(title = "Linearity Check: Dem.Rep vs Logit", x = "Dem.Rep", y = "Logit")

# Function to compute empirical logit and create the plot
empirical_logit_tidy <- function(data, predictor, response) {
  # Create bins for the predictor and calculate empirical logits
  emp_log <- data %>%
    mutate(BinnedPredictor = cut(!!sym(predictor), breaks = 10)) %>% # Bin the predictor
    group_by(BinnedPredictor) %>%
    summarize(
      MeanPredictor = mean(!!sym(predictor), na.rm = TRUE), # Mean of the predictor in each bin
      SuccessCount = sum(!!sym(response)), # Count of successes
      TotalCount = n(), # Total count in each bin
      ProportionSuccess = SuccessCount / TotalCount # Proportion of successes
    ) %>%
    mutate(
      OddsSuccess = ProportionSuccess / (1 - ProportionSuccess), # Odds of success
      EmpiricalLogit = log(OddsSuccess) # Logit (log-odds)
    ) %>%
    filter(!is.infinite(EmpiricalLogit)) # Remove bins with infinite logits
}
```


```{r}
# Plot empirical logit vs predictor


## EDIT THIS FUNCTION BELOW BASED ON THE empirical_logit_tidy FUNCTION ABOVE

# ggplot(emp_log, aes(x = MeanPredictor, y = EmpiricalLogit)) +
#   geom_point() +
#   geom_smooth(method = "loess", se = FALSE, color = "blue") +
#   labs(title = paste("Empirical Logit Plot for", predictor),
#        x = predictor,
#        y = "Empirical Logit") +
#   theme_minimal()

```



```{r}
# Generate empirical logit plots for each predictor
empirical_logit_tidy(Election08, "Income", "ObamaWin")
empirical_logit_tidy(Election08, "HS", "ObamaWin")
empirical_logit_tidy(Election08, "BA", "ObamaWin")
empirical_logit_tidy(Election08, "Dem.Rep", "ObamaWin")

```

Thus, the linearity condition is satisfied since there exists a linear relationship between the log(odds) and the predictor(s).

```{r}
# Correlation matrix
cor_matrix <- cor(Election08[, c("Income", "HS", "BA", "Dem.Rep")])
print(cor_matrix)

# Variance Inflation Factor (VIF)
vif(lm(Dem.Rep ~ Income + HS + BA, data = Election08))

```

## Logistic regression models
```{r}
# Logistic regression for each predictor
models <- list(
  Income = glm(ObamaWin ~ Income, data = Election08, family = "binomial"),
  HS = glm(ObamaWin ~ HS, data = Election08, family = "binomial"),
  BA = glm(ObamaWin ~ BA, data = Election08, family = "binomial"),
  DemRep = glm(ObamaWin ~ Dem.Rep, data = Election08, family = "binomial")
)

```


```{r}
# Summarize models
lapply(models, summary)

# optimized model
final_model <- glm(ObamaWin ~ Income + HS + BA + Dem.Rep, data = Election08, family = "binomial")
summary(final_model)

```


## Model Comparison
```{r}
compare_metrics <- function(model, data, response) {
  # Predicted probabilities
  preds <- predict(model, type = "response")
  # Binary classification (threshold = 0.5)
  preds_class <- ifelse(preds > 0.5, 1, 0)
  
  # SSE: Sum of Squares for Error
  sse <- sum((data[[response]] - preds_class)^2)
  
  # SSM: Sum of Squares for Model
  mean_response <- mean(data[[response]])
  ssm <- sum((preds_class - mean_response)^2)
  
  # SSR: Total Sum of Squares
  ssr <- sum((data[[response]] - mean_response)^2)
  
  # R-squared
  r_squared <- 1 - (sse / ssr)
  
  # Adjusted R-squared
  n <- nrow(data)
  p <- length(coef(model)) - 1
  adj_r_squared <- 1 - ((1 - r_squared) * (n - 1) / (n - p - 1))
  
  list(SSE = sse, SSM = ssm, SSR = ssr, R_squared = r_squared, Adjusted_R_squared = adj_r_squared)
}

```
##G-tests

```{r}
##G-test for HS
G <- 69.737 - 65.741
df <- 1
p_value_g_test <- pchisq(G, df, lower.tail = FALSE)
print(p_value_g_test)

##G-test for BA
G <- 69.737 - 49.689
df <- 1
p_value_g_test <- pchisq(G, df, lower.tail = FALSE)
print(p_value_g_test)

##G-test for DemRep
G <- 69.737 - 27.167
df <- 1
p_value_g_test <- pchisq(G, df, lower.tail = FALSE)
print(p_value_g_test)

##G-test for Final Model
G <- 69.737 - 9.7252
df <- 4
p_value_g_test <- pchisq(G, df, lower.tail = FALSE)
print(p_value_g_test)

```


# Results

## Interpretation

# Suggestions for future research

# Conclusion
